#  FlameTickets: Arquitectura de Microservicios Resiliente

**Proyecto Final & v1**
**Materia:** Computaci贸n Tolerante a Fallas
**Alumno:** [Daniel Gaitan Chavez]
**Semestre:** 2025B

---

##  Cumplimiento de Objetivos T茅cnicos

Este proyecto implementa una soluci贸n de venta de boletos distribuida, siguiendo estrictamente los principios de microservicios y tolerancia a fallos.

### 1. Divisi贸n en Servicios Independientes
La aplicaci贸n se desacopl贸 en dos m贸dulos aut贸nomos con responsabilidades 煤nicas:
* **Frontend Service (Node.js/Express):** Gateway de cara al usuario. Maneja la interfaz web y la orquestaci贸n de llamadas.
* **Logic Service (Python/Flask):** N煤cleo de procesamiento. Gestiona la l贸gica de negocio de las 贸rdenes.
* *Justificaci贸n:* El uso de lenguajes distintos demuestra la independencia tecnol贸gica de cada componente.

Seguridad y Aislamiento de Red Aunque el servicio es p煤blico, se implement贸 una estrategia de Seguridad por Aislamiento (Network Isolation):

Internal DNS Resolution: La comunicaci贸n entre microservicios no viaja por internet p煤blica, sino a trav茅s de la red interna del cl煤ster (ClusterIP), cifrada por la capa de red de Kubernetes.

Backend Protegido: El servicio Logic no tiene IP p煤blica; solo es accesible a trav茅s del Frontend, previniendo ataques directos a la API de l贸gica.

### 2. Contenerizaci贸n con Docker
Cada microservicio cuenta con su propio `Dockerfile`, utilizando im谩genes base ligeras (`node:18-alpine` y `python:3.9-slim`) para garantizar portabilidad y consistencia entre entornos de desarrollo y producci贸n.

### 3. Orquestaci贸n con Kubernetes
Se utiliza un cl煤ster de Kubernetes para gestionar el ciclo de vida de la aplicaci贸n.
* **Deployments:** Garantizan el estado deseado de la aplicaci贸n.
* **Self-Healing:** Configuraci贸n autom谩tica para reiniciar contenedores fallidos.

### 4. Comunicaci贸n entre Microservicios
La interacci贸n se realiza mediante **API REST** sobre HTTP.
* Se utiliza el DNS interno de Kubernetes (`flame-logic-service`) para el descubrimiento de servicios, eliminando la dependencia de IPs fijas.

### 5. Monitorizaci贸n y Observabilidad

Para mantener la arquitectura ligera ("Lightweight Architecture"), optamos por la Observabilidad Nativa de Kubernetes en lugar de herramientas externas pesadas como Istio.

Centralizaci贸n de Logs: Kubernetes recolecta los streams STDOUT/STDERR de todos los contenedores.

Inspecci贸n en Tiempo Real: Uso de kubectl logs para trazabilidad inmediata de errores y confirmaci贸n de transacciones (HTTP 200/500), como se muestra en las evidencias.

Implementaci贸n de **Logging Estructurado** en salida est谩ndar (STDOUT/STDERR).
* Kubernetes agrega los logs de todos los pods, permitiendo inspeccionar el tr谩fico en tiempo real mediante `kubectl logs -l app=flame-frontend`.

![Logs de Trafico](./img/warning1.png)


### 6. Automatizaci贸n (CI/CD)
Se incluyen scripts de automatizaci贸n (`deploy.ps1`) que estandarizan el proceso de:
1.  Construcci贸n de im谩genes (Build).
2.  Empaquetado.
3.  Despliegue al cl煤ster (Deploy).

### 7. Dise帽o Escalable y Resiliente
* **Escalabilidad Horizontal:** El servicio de l贸gica est谩 configurado con `replicas: 2` para balancear la carga.
* **Resiliencia:** El Frontend implementa manejo de errores (Try/Catch) para degradar el servicio elegantemente si el Backend falla, mostrando mensajes amigables al usuario en lugar de colapsar.

### 8. Seguridad y Aislamiento
* **Network Policy:** El servicio de l贸gica (`ClusterIP`) est谩 aislado de internet; solo acepta peticiones del Frontend dentro de la red privada del cl煤ster. Solo el Frontend expone puerto p煤blico (`LoadBalancer`).

### 9. Ingenier铆a del Caos (Chaos Engineering)
El sistema fue sometido a pruebas de estr茅s y fallos inyectados:
* **Nivel Infraestructura:** Eliminaci贸n manual de Pods en ejecuci贸n para verificar la regeneraci贸n autom谩tica por parte del ReplicaSet.
* **Nivel Aplicaci贸n:** Inyecci贸n de c贸digo en Python (`random fail`) que simula errores 500 aleatorios para probar la robustez del cliente.

---

##  Gu铆a de Despliegue (Tutorial)

### Prerrequisitos
* Docker Desktop habilitado con Kubernetes.

  ![imagen](./img/helloDocker.png)

* Terminal (PowerShell o Bash).

![Docker Desktop Running](./img/ambos_corriendo.png)

### Paso 1: Clonar y Ubicarse
```bash
git clone [https://github.com/DanielGaitan1/Computacion_tolerante_fallas/tree/main]
cd Computacion_tolerante_fallas/Proyecto_Final 
```
![Estructura del Proyecto](./img/lsRepositorio.png)
### Paso 2: Ejecutar Script de Automatizaci贸n
Hemos creado un script que construye las im谩genes y despliega los servicios autom谩ticamente.

./deploy.ps1
(Alternativamente, despliegue manual):

```Bash
docker build -t flame-frontend:v1 ./service-frontend
docker build -t flame-logic:v1 ./service-logic
kubectl apply -f ./k8s/main-deployment.yaml
```

**Construyendo Frontend:**
![Build Frontend](./img/azulBuildFrontend.png)

**Construyendo Logic Service:**
![Build Logic](./img/azul1.png)


### Paso 3: Verificar y Probar
Verificar que los pods est茅n corriendo: kubectl get pods

![Deploy en Kubernetes](./img/applyCreated.png)


1. **Verificar que los pods est茅n corriendo:**
   `kubectl get pods`
   
   ![Pods Corriendo](./img/get_pods.png)

2. **Acceder a la web:**
   Entra a [http://localhost](http://localhost) en tu navegador. Deber谩s ver la interfaz del concierto.
   
   ![Interfaz Web](./img/web2.png)

3. **Simular una compra (Prueba de conexi贸n):**
   Haz clic en "Comprar Boleto". Si todo funciona, ver谩s el ID del ticket y qu茅 contenedor lo proces贸.
  
   ![Compra Exitosa](./img/web3.png)

4. **Verificar logs en tiempo real:**
   `kubectl logs -l app=flame-logic`
   
   ![Logs de Trafico](./img/warning1.png)

Acceder a la web: http://localhost

---

## И Evidencias de Resiliencia (Chaos Testing)

El objetivo es demostrar que el sistema se recupera autom谩ticamente tras un fallo cr铆tico.

### Prueba 1: Eliminaci贸n Manual de un Pod (Pod Deletion)

**1. El Ataque:**
Eliminamos manualmente un pod del servicio de l贸gica para simular un "crash" o fallo fatal.
Comando: `kubectl delete pod logic-deployment-xxxxx`

![Eliminando Pod](./img/delete1.png)

**2. La Recuperaci贸n (Self-Healing):**
Kubernetes detecta que falta una r茅plica y crea una nueva inmediatamente.
En la siguiente captura se observa:
* Pods antiguos (6 minutos de vida).
* **Nuevo Pod** (39 segundos de vida) creado autom谩ticamente para reemplazar al eliminado.

![Recuperacion Automatica](./img/getPods2.png)


---

Write-Host " Accede en: http://localhost"


---

## Ч Limpieza del Entorno
Para detener y eliminar todos los servicios del cl煤ster:
`kubectl delete -f ./k8s/main-deployment.yaml`

![Limpieza](./img/delete2.png)

---
##  Presentaci贸n Ejecutiva
Puedes consultar la presentaci贸n detallada del proyecto y la arquitectura aqu铆:
[ Ver Presentaci贸n del Proyecto (PDF)](./Presentacion_Proyecto.pdf)