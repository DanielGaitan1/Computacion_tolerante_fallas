# üõ°Ô∏è Sistema de Clasificaci√≥n de Leads Tolerante a Fallas (n8n + Docker)
### Proyecto de Automatizaci√≥n e Ingenier√≠a de Software - Autocristales Gaitan

![Status](https://img.shields.io/badge/Status-Completed-green)
![Tech](https://img.shields.io/badge/Stack-n8n%20|%20Docker%20|%20JS-blue)


## üìã Resumen Ejecutivo
Este proyecto implementa una arquitectura automatizada para la gesti√≥n de clientes potenciales (Leads). El sistema ingesta mensajes, procesa la intenci√≥n del cliente y notifica al operador v√≠a Telegram.

El enfoque principal del proyecto fue aplicar principios de **Computaci√≥n Tolerante a Fallas**, dise√±ando un sistema que resiste la ca√≠da de servicios externos (APIs de IA) mediante mecanismos de simulaci√≥n (Mocking) y degradaci√≥n agraciada.

---

## üé• Demostraci√≥n del Sistema

| Notificaci√≥n en Tiempo Real (M√≥vil) | Ejecuci√≥n del Flujo (Backend) |
| :---: | :---: |
| ![Demo Mobile](./evidencias/demo_mobile.gif) | ![Demo Workflow](./evidencias/demo_workflow.gif) |

---

## üèóÔ∏è 1. Infraestructura y Despliegue (Docker)
Para garantizar la portabilidad y el aislamiento, el sistema se despleg√≥ utilizando contenedores Docker. Se configuraron vol√∫menes persistentes para evitar p√©rdida de datos ante reinicios.

**Comando de instalaci√≥n:**
![Docker Install](./evidencias/1_docker_install.png)

**Estado del Contenedor:**
> Servicio n8n operando en puerto 5678 con consumo de recursos estable.
![Docker Status](./evidencias/2_docker_status.png)

---

## üß™ 2. Ingesta de Datos
El flujo inicia con un **Webhook (POST)** que recibe datos estructurados desde herramientas de prueba (Postman) o formularios web.

**Trigger (Postman):**
![Postman](./evidencias/4_postman_trigger.png)

**Recepci√≥n Exitosa:**
![Webhook Payload](./evidencias/5_webhook_received.png)

---

## ‚ö†Ô∏è 3. El Desaf√≠o: Fallo en la API de Inteligencia Artificial
Siguiendo la r√∫brica de *"Investigar, Experimentar y Fallar"*, se intent√≥ integrar la API de **Google Gemini (1.5 Flash)** para realizar procesamiento de lenguaje natural (NLP).

A pesar de configurar correctamente las credenciales y headers:
![API Keys](./evidencias/6_google_keys.png)

**Se document√≥ un fallo cr√≠tico de disponibilidad (Error 404):**
La API rechaz√≥ las conexiones debido a inconsistencias en el versionado de los modelos, representando un punto √∫nico de fallo para el sistema.
![Error 404](./evidencias/7_error_api_404.png)
![Node Failure](./evidencias/8_node_failure.png)

---

## üõ°Ô∏è 4. Soluci√≥n: Arquitectura Tolerante a Fallas
Para evitar que el negocio se detuviera por el fallo de la IA, se implement√≥ un patr√≥n de **Mocking & Fallback**.

**Estrategia:**
Se reemplaz√≥ la dependencia externa por un m√≥dulo l√≥gico interno (JavaScript Node) que:
1.  **Simula** la estructura de respuesta de la IA.
2.  **Garantiza** la integridad de los datos (`intencion`, `auto`).
3.  **Permite** que el flujo contin√∫e hacia la notificaci√≥n.

**Arquitectura Resiliente Resultante:**
![Final Flow](./evidencias/9_resilient_flow.png)

---

## üî¨ 5. Experimentaci√≥n Extendida
Como parte de la investigaci√≥n, se exploraron capacidades avanzadas de n8n, incluyendo la ejecuci√≥n de comandos de sistema (`Execute Command`) para interactuar con el sistema de archivos local y generar logs en CSV.

**Flujo Extendido (Experimentaci√≥n):**
![Experimentation](./evidencias/10_experimentation.png)

**Interacci√≥n con Sistema de Archivos:**
![Local Files](./evidencias/11_local_files.png)

---

## ‚úÖ 6. Resultado Final
Gracias a la implementaci√≥n de tolerancia a fallas, el sistema es capaz de entregar la alerta al operador humano sin interrupciones, independientemente del estado de la IA.

**Configuraci√≥n del Bot:**
![Bot Setup](./evidencias/3_bot_creation.png)

**Notificaci√≥n Exitosa en Producci√≥n:**
![Success](./evidencias/12_success_alert.png)

---

## üß† An√°lisis Te√≥rico: Model Context Protocol (MCP)
*Requisito de Investigaci√≥n*

La problem√°tica de conexi√≥n experimentada (Errores 404/Headers) resalta la necesidad de est√°ndares como **MCP**.

**¬øC√≥mo MCP resolver√≠a esto?**
En lugar de configurar manualmente peticiones HTTP fr√°giles, un servidor MCP expondr√≠a recursos estandarizados (ej: `inventario_cristales`). El LLM interactuar√≠a con estos recursos mediante un protocolo seguro y universal, eliminando la complejidad de las integraciones API directas y reduciendo la superficie de error humano.