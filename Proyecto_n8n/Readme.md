# üõ°Ô∏è Sistema de Clasificaci√≥n de Leads Tolerante a Fallas (n8n + Docker)
### Proyecto de Automatizaci√≥n e Ingenier√≠a de Software - Autocristales Gaitan

![Status](https://img.shields.io/badge/Status-Completed-green)
![Tech](https://img.shields.io/badge/Stack-n8n%20|%20Docker%20|%20JS-blue)

Daniel Gaitan Chavez
2025B
INCO


## üìã Resumen Ejecutivo
Este proyecto implementa una arquitectura automatizada para la gesti√≥n de clientes potenciales (Leads). El sistema ingesta mensajes, procesa la intenci√≥n del cliente y notifica al operador v√≠a Telegram.

El enfoque principal del proyecto fue aplicar principios de **Computaci√≥n Tolerante a Fallas**, dise√±ando un sistema que resiste la ca√≠da de servicios externos (APIs de IA) mediante mecanismos de simulaci√≥n (Mocking) y degradaci√≥n agraciada.

---

## üé• Demostraci√≥n del Sistema

| Notificaci√≥n en Tiempo Real (M√≥vil) | Ejecuci√≥n del Flujo (Backend) |
| :---: | :---: |
| ![Demo Mobile](./evidencias/demo_mobile.gif) | ![Demo Workflow](./evidencias/demo_workflow.gif) |

---

## üèóÔ∏è 1. Infraestructura y Despliegue (Docker)
Para garantizar la portabilidad y el aislamiento, el sistema se despleg√≥ utilizando contenedores Docker. Se configuraron vol√∫menes persistentes para evitar p√©rdida de datos ante reinicios.

**Comando de instalaci√≥n:**
![Docker Install](./evidencias/1_docker_install.png)

**Estado del Contenedor:**
> Servicio n8n operando en puerto 5678 con consumo de recursos estable.
![Docker Status](./evidencias/2_docker_status.png)

---

## üß™ 2. Ingesta de Datos
El flujo inicia con un **Webhook (POST)** que recibe datos estructurados desde herramientas de prueba (Postman) o formularios web.

**Trigger (Postman):**

![Postman](./evidencias/4_postman_trigger.png)

**Recepci√≥n Exitosa:**

![Webhook Payload](./evidencias/5_webhook_received.png)

---

## ‚ö†Ô∏è 3. El Desaf√≠o: Fallo en la API de Inteligencia Artificial
Siguiendo la r√∫brica de *"Investigar, Experimentar y Fallar"*, se intent√≥ integrar la API de **Google Gemini (1.5 Flash)** para realizar procesamiento de lenguaje natural (NLP).

A pesar de configurar correctamente las credenciales y headers:
![API Keys](./evidencias/6_google_keys.png)

**Se document√≥ un fallo cr√≠tico de disponibilidad (Error 404):**
La API rechaz√≥ las conexiones debido a inconsistencias en el versionado de los modelos, representando un punto √∫nico de fallo para el sistema.
![Error 404](./evidencias/7_error_api_404.png)
![Node Failure](./evidencias/8_node_failure.png)

---

## üõ°Ô∏è 4. Soluci√≥n: Arquitectura Tolerante a Fallas
Para evitar que el negocio se detuviera por el fallo de la IA, se implement√≥ un patr√≥n de **Mocking & Fallback**.

**Estrategia:**
Se reemplaz√≥ la dependencia externa por un m√≥dulo l√≥gico interno (JavaScript Node) que:
1.  **Simula** la estructura de respuesta de la IA.
2.  **Garantiza** la integridad de los datos (`intencion`, `auto`).
3.  **Permite** que el flujo contin√∫e hacia la notificaci√≥n.

**Arquitectura Resiliente Resultante:**
![Final Flow](./evidencias/9_resilient_flow.png)

---

## üî¨ 5. Experimentaci√≥n Extendida
Como parte de la investigaci√≥n, se exploraron capacidades avanzadas de n8n, incluyendo la ejecuci√≥n de comandos de sistema (`Execute Command`) para interactuar con el sistema de archivos local y generar logs en CSV.

**Flujo Extendido (Experimentaci√≥n):**
![Experimentation](./evidencias/10_experimentation.png)

**Interacci√≥n con Sistema de Archivos:**
![Local Files](./evidencias/11_local_files.png)

---

## ‚úÖ 6. Resultado Final
Gracias a la implementaci√≥n de tolerancia a fallas, el sistema es capaz de entregar la alerta al operador humano sin interrupciones, independientemente del estado de la IA.

**Configuraci√≥n del Bot:**
![Bot Setup](./evidencias/3_bot_creation.png)

**Notificaci√≥n Exitosa en Producci√≥n:**
![Success](./evidencias/12_success_alert.png)

---

## üß† An√°lisis Te√≥rico: Model Context Protocol (MCP)
*Requisito de Investigaci√≥n*

La problem√°tica de conexi√≥n experimentada (Errores 404/Headers) resalta la necesidad de est√°ndares como **MCP**.

**¬øC√≥mo MCP resolver√≠a esto?**
En lugar de configurar manualmente peticiones HTTP fr√°giles, un servidor MCP expondr√≠a recursos estandarizados (ej: `inventario_cristales`). El LLM interactuar√≠a con estos recursos mediante un protocolo seguro y universal, eliminando la complejidad de las integraciones API directas y reduciendo la superficie de error humano.

## üìë Anexo: Reflexi√≥n T√©cnica y Respuestas a la Gu√≠a
A continuaci√≥n, se da respuesta a las interrogantes planteadas en la asignaci√≥n, bas√°ndonos en la experiencia de desarrollo de este proyecto.

### 1. Fundamentos de Automatizaci√≥n (n8n)

**¬øQu√© es n8n? ¬øC√≥mo se compara con herramientas como Zapier o Make?**
n8n es una herramienta de automatizaci√≥n de flujos de trabajo basada en nodos. A diferencia de Zapier o Make (que son SaaS cerrados), n8n es "fair-code", lo que permite alojarlo en nuestros propios servidores (Self-hosted) usando Docker. Esto ofrece mayor privacidad de datos y elimina los l√≠mites estrictos de ejecuciones por costo que tienen las otras plataformas.

**¬øQu√© "disparador" (trigger) iniciar√° mi flujo?**
El flujo inicia con un nodo **Webhook**. Este escucha peticiones HTTP `POST` en una URL espec√≠fica, actuando como la "puerta de entrada" para los datos que vienen desde el formulario web o Postman.

**¬øQu√© acciones deben suceder despu√©s?**
El flujo sigue una l√≥gica lineal:
1.  **Recepci√≥n:** El Webhook acepta el JSON.
2.  **Procesamiento:** Se intenta analizar el texto (originalmente con IA, luego con Mock en JS).
3.  **Salida:** Se formatea el mensaje y se env√≠a al Bot de Telegram.

**¬øC√≥mo paso datos de un nodo a otro?**
En n8n, los datos fluyen en formato JSON. Cada nodo recibe la salida ("Output") del anterior como su entrada ("Input"). Utilizamos expresiones para referenciar datos espec√≠ficos, por ejemplo: `{{ $json.body.mensaje }}`.

**¬øMi flujo funciona? ¬øQu√© pasa si falla un paso?**
El flujo es funcional. Sin embargo, descubrimos que las dependencias externas (APIs de IA) pueden fallar. Si un paso falla, el flujo se detiene y marca error. Para mitigar esto, implementamos l√≥gica de respaldo (Fallback) en c√≥digo, asegurando que el proceso sea **tolerante a fallas**.

---

### 2. Model Context Protocol (MCP) e Inteligencia Artificial

**¬øQu√© problema resuelve el Model Context Protocol? ¬øPor qu√© es √∫til?**
El MCP resuelve la fragmentaci√≥n en la conexi√≥n entre LLMs y datos. Actualmente, para conectar Gemini con n8n tuvimos que configurar manualmente Headers, URLs y Payloads (lo que caus√≥ errores 404). MCP estandariza esto, permitiendo una conexi√≥n universal y segura sin "reinventar la rueda" en cada integraci√≥n.

**¬øCu√°l es la diferencia entre un "Cliente" y un "Servidor" MCP?**
* **Cliente MCP:** Es la interfaz donde vive el usuario o la IA (ej. Claude Desktop o el IDE). Es quien *pide* la informaci√≥n.
* **Servidor MCP:** Es el programa que tiene acceso a los datos (ej. nuestro sistema de Autocristales). Es quien *entrega* las herramientas o recursos al cliente.

**¬øC√≥mo decide un LLM cu√°ndo y qu√© herramienta MCP usar?**
El LLM analiza la descripci√≥n de las herramientas disponibles en el servidor. Si el prompt del usuario dice "Dame el precio del parabrisas", y el servidor expone una herramienta llamada `consultar_precio`, el LLM decide inteligentemente invocar esa funci√≥n espec√≠fica.

---

### 3. Integraci√≥n y L√≥gica del Proyecto

**¬øQu√© parte de mi proceso se beneficia de la "inteligencia"?**
La clasificaci√≥n de la intenci√≥n (`intencion`) y la extracci√≥n de entidades (`marca`, `modelo`, `a√±o`). Un sistema tradicional necesitar√≠a muchos `if/else` complejos para entender "mi jetta se rompi√≥" vs "cotiza un jetta". El LLM lo entiende nativamente.

**¬øQu√© datos necesita el LLM para hacer su trabajo? ¬øC√≥mo se los proporciona N8N?**
El LLM necesita el mensaje crudo del cliente. n8n se lo proporciona inyectando la variable del Webhook dentro del cuerpo (Body) de la petici√≥n HTTP o dentro del script de simulaci√≥n.

**¬øC√≥mo estructuro el "prompt" que N8N enviar√° al LLM para obtener la respuesta deseada?**
Utilizamos un "System Prompt" que define el rol y el formato de salida estricto.
* *Ejemplo:* "Eres un experto en ventas. Analiza el mensaje y devuelve √öNICAMENTE un JSON con esta estructura...".

**¬øQu√© hago con la respuesta del LLM? ¬øC√≥mo la uso en el siguiente paso de mi automatizaci√≥n?**
La respuesta (un objeto JSON) se mapea a los campos del nodo de Telegram.
* El campo `auto.marca` del JSON se convierte en texto visible en la notificaci√≥n: "üöó Auto: Nissan".